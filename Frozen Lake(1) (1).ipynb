{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen Lake\n",
    "===\n",
    "The goal of this computer assignment is to get familiar with OpenAI Gym, implement value iteration and policy iteration.\n",
    "\n",
    "Problem Description\n",
    "---\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. For more information visit https://gym.openai.com.\n",
    "\n",
    "In this computer assigment, you'll get familiar with Frozen Lake environment and implement value and policy iteration algorithms. Frozen Lake is an environment where the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. For more information please visit https://gym.openai.com/envs/FrozenLake8x8-v0/.\n",
    "\n",
    "Your Job\n",
    "---\n",
    "1. Get started with gym by following the steps here https://gym.openai.com/docs/.\n",
    "2. Read https://gym.openai.com/envs/FrozenLake8x8-v0/ to get familiar with the environment, states, reward function, etc.\n",
    "3. Implement the $\\texttt{value_iteration}$ function below.\n",
    "4. Implement the $\\texttt{policy_iteration}$ function below.\n",
    "5. Answer the questions (By double click on the cell you can edit the cell and put your answer below each question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "Value-iteration converged at iteration# 315.\n",
      "Value iteration took 0.641289234161377 seconds.\n",
      "Average score =  0.00628743678838866\n",
      "---------- Gamma=0.95 ----------\n",
      "Value-iteration converged at iteration# 497.\n",
      "Value iteration took 0.7956991195678711 seconds.\n",
      "Average score =  0.0479452941660768\n",
      "---------- Gamma=0.99 ----------\n",
      "Value-iteration converged at iteration# 1126.\n",
      "Value iteration took 1.6570303440093994 seconds.\n",
      "Average score =  0.4065790712237092\n",
      "---------- Gamma=0.9999 ----------\n",
      "Value-iteration converged at iteration# 2336.\n",
      "Value iteration took 3.995661735534668 seconds.\n",
      "Average score =  0.865706696499763\n",
      "---------- Gamma=1 ----------\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Value iteration took 3.9234561920166016 seconds.\n",
      "Average score =  0.866\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "def run_episode(env, policy, gamma, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma=gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma, epsilon=1e-20, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements value iteration algorithm for the infinite\n",
    "    horizon discounted MDPs. If the sup norm of v_k - v_{k-1} is less than\n",
    "    epsilon or number of iterations reaches max_iterations, it should return\n",
    "    the value function.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    ########################### Your Code Here ###########################\n",
    "    # Hint: see implementation of extract_policy\n",
    "    for i in range(max_iterations):\n",
    "        prv_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + gamma * prv_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.linalg.norm((prv_v - v),np.inf) <= epsilon):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Value iteration took {0} seconds.\".format(end - start))\n",
    "    return v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        optimal_v = value_iteration(env, gamma);\n",
    "        policy = extract_policy(optimal_v, gamma)\n",
    "        policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "        print('Average score = ', policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Gamma=0.9 ----------\n",
      "Policy-Iteration converged at step 5.\n",
      "Policy iteration took 0.4083437919616699 seconds.\n",
      "Average scores =  0.00594476566695997\n",
      "---------- Gamma=0.95 ----------\n",
      "Policy-Iteration converged at step 3.\n",
      "Policy iteration took 0.44911885261535645 seconds.\n",
      "Average scores =  0.048241249527001685\n",
      "---------- Gamma=0.99 ----------\n",
      "Policy-Iteration converged at step 8.\n",
      "Policy iteration took 2.030360221862793 seconds.\n",
      "Average scores =  0.37057608411841303\n",
      "---------- Gamma=0.9999 ----------\n",
      "Policy-Iteration converged at step 12.\n",
      "Policy iteration took 7.533506870269775 seconds.\n",
      "Average scores =  0.861386129235637\n",
      "---------- Gamma=1 ----------\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy iteration took 4.60036563873291 seconds.\n",
      "Average scores =  0.88\n"
     ]
    }
   ],
   "source": [
    "def compute_policy_v(env, policy, gamma):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    This function implements policy iteration algorithm.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    ########################### Your Code Here ###########################\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################### End of your code #########################\n",
    "    end = time.time()\n",
    "    print(\"Policy iteration took {0} seconds.\".format(end - start))\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1111)\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    for gamma in [.9, .95, .99, .9999, 1]:\n",
    "        print(\"-\"*10, \"Gamma={0}\".format(gamma) ,\"-\"*10)\n",
    "        env = gym.make(env_name)\n",
    "        optimal_policy = policy_iteration(env, gamma=gamma)\n",
    "        scores = evaluate_policy(env, optimal_policy, gamma=gamma)\n",
    "        print('Average scores = ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "--- \n",
    "\n",
    "#### 1. How many iterations did it take for the value iteration to converge? How about policy iteration?\n",
    "For gamma=1, value iteration converges in 2357 steps & policy iteration converges in 6 steps. For gamma=0.9, value iteration converges in 315 steps & policy iteration converges in 5 steps. Number of iterations are printed out for every gamma value & iterative method.\n",
    "\n",
    "#### 2. How much time did it take for the value iteration to converge? How about the policy iteration?\n",
    "For gamma=1, 4.749seconds for value iteration & 3.478seconds for policy iteration. For different gamma values & algorithm, time required for convergence is printed.\n",
    "\n",
    "#### 3. Which algorithm is faster? Why?\n",
    "Policy iteration algorithm is faster because it converges in finite time since it finds optimal policy directly unlike value iteration algorithm which converges asymptotically. Value iteration algorithm tries to find optimal value function from which we need to find the optimal policy.\n",
    "\n",
    "#### 4. How does the average score change as $\\gamma$ gets closer to 1? Why?\n",
    "For gamma =1, average score of 0.864 for value iteration & 0.88 for policy iteration. The average score changes drastically(increases) when gamma approches 1 beacause the rewards of high step_idx is taken as it is without multiplying by gamma to the power of (num of step_idx). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
